r"Implementations of modules to be used inside of `torch.nn.Sequential` instead of the `forward` method."
import math
import warnings

import torch
from torch import Tensor
from torch.nn import Module, Parameter
from torch.autograd import Function
from torch.nn.functional import relu, cross_entropy, cosine_similarity, binary_cross_entropy

from collections.abc import Callable

class Permute(Module):
    """Helper class to be used inside torch.nn.Sequential."""
    def __init__(self, permutation: tuple[int]):
        super().__init__()
        self.p = permutation
    
    def forward(self, x: Tensor) -> Tensor:
        return x.permute(self.p)
    
class Scaler(Module):
    """Helper class to be used inside torch.nn.Sequential"""
    def __init__(self, dim: int, layer_scale_init_value) -> None:
        super().__init__()
        self.gamma = Parameter(layer_scale_init_value * torch.ones((dim)), requires_grad=True) if layer_scale_init_value else None
        
    def forward(self, x: Tensor) -> Tensor:
        if self.gamma is not None:
            return self.gamma * x
        return self.gamma


class DropPath(Module):
    """Drop paths (Stochastic Depth) per sample  (when applied in main path of residual blocks).
    """
    def __init__(self, drop_prob: float = 0., scale_by_keep: bool = True):
        super().__init__()
        self.drop_prob = drop_prob
        self.scale_by_keep = scale_by_keep

    def drop_path(self, x, drop_prob: float = 0., training: bool = False, scale_by_keep: bool = True):
        if drop_prob == 0. or not training:
            return x
        keep_prob = 1 - drop_prob
        shape = (x.shape[0],) + (1,) * (x.ndim - 1)
        random_tensor = x.new_empty(shape).bernoulli_(keep_prob)
        if keep_prob > 0.0 and scale_by_keep:
            random_tensor.div_(keep_prob)
        return x * random_tensor

    def forward(self, x):
        return self.drop_path(x, self.drop_prob, self.training, self.scale_by_keep)
    

def _trunc_normal_(tensor, mean, std, a, b):
    # Cut & paste from PyTorch official master until it's in a few official releases - RW
    # Method based on https://people.sc.fsu.edu/~jburkardt/presentations/truncated_normal.pdf
    def norm_cdf(x):
        # Computes standard normal cumulative distribution function
        return (1. + math.erf(x / math.sqrt(2.))) / 2.

    if (mean < a - 2 * std) or (mean > b + 2 * std):
        warnings.warn("mean is more than 2 std from [a, b] in nn.init.trunc_normal_. "
                      "The distribution of values may be incorrect.",
                      stacklevel=2)

    # Values are generated by using a truncated uniform distribution and
    # then using the inverse CDF for the normal distribution.
    # Get upper and lower cdf values
    l = norm_cdf((a - mean) / std)
    u = norm_cdf((b - mean) / std)

    # Uniformly fill tensor with values from [l, u], then translate to
    # [2l-1, 2u-1].
    tensor.uniform_(2 * l - 1, 2 * u - 1)

    # Use inverse cdf transform for normal distribution to get truncated
    # standard normal
    tensor.erfinv_()

    # Transform to proper mean, std
    tensor.mul_(std * math.sqrt(2.))
    tensor.add_(mean)

    # Clamp to ensure it's in the proper range
    tensor.clamp_(min=a, max=b)
    return tensor

def trunc_normal_(tensor, mean=0., std=1., a=-2., b=2.):
    # type: (Tensor, float, float, float, float) -> Tensor
    r"""Fills the input Tensor with values drawn from a truncated
    normal distribution. The values are effectively drawn from the
    normal distribution :math:`\mathcal{N}(\text{mean}, \text{std}^2)`
    with values outside :math:`[a, b]` redrawn until they are within
    the bounds. The method used for generating the random values works
    best when :math:`a \leq \text{mean} \leq b`.

    NOTE: this impl is similar to the PyTorch trunc_normal_, the bounds [a, b] are
    applied while sampling the normal with mean/std applied, therefore a, b args
    should be adjusted to match the range of mean, std args.

    Args:
        tensor: an n-dimensional `torch.Tensor`
        mean: the mean of the normal distribution
        std: the standard deviation of the normal distribution
        a: the minimum cutoff value
        b: the maximum cutoff value
    Examples:
        >>> w = torch.empty(3, 5)
        >>> nn.init.trunc_normal_(w)
    """
    with torch.no_grad():
        return _trunc_normal_(tensor, mean, std, a, b)
    
# https://github.com/tadeephuy/GradientReversal/
class GradientReversal(Function):
    @staticmethod
    def forward(ctx, x, alpha):
        ctx.save_for_backward(x, alpha)
        return x
    
    @staticmethod
    def backward(ctx, grad_output):
        grad_input = None
        _, alpha = ctx.saved_tensors
        if ctx.needs_input_grad[0]:
            grad_input = - alpha*grad_output
        return grad_input, None

revgrad = GradientReversal.apply
class GradientReversal(Module):
    def __init__(self, alpha):
        super().__init__()
        self.alpha = torch.tensor(alpha, requires_grad=False)

    def forward(self, x):
        return revgrad(x, self.alpha)
    
class ContrastiveLoss(Module):
    def __init__(self, positive_margin: float = 0.0, negative_margin: float = 1.0,
                 reduction: Callable[[Tensor], Tensor] = torch.mean) -> None:
        super().__init__()
        self.pm = positive_margin
        self.nm = negative_margin
        self.reduction = None
        self.reduction = reduction
    
    def forward(self, anchor: Tensor, positive: Tensor, negative: Tensor) -> Tensor:
        pd = torch.linalg.norm(anchor - positive, dim=0)
        nd = torch.linalg.norm(anchor - negative, dim=0)
        loss = relu(pd - self.pm) + relu(self.nm - nd)
        return self.reduction(loss)
    
class DomainLoss(Module):
    def __init__(self, n_classes: int) -> None:
        super().__init__()
        self.random_loss = math.log(n_classes)
    
    def forward(self, predictions: Tensor, targets: Tensor) -> Tensor:
        loss = cross_entropy(predictions, targets)
        return loss.clip_(min=0.0, max=self.random_loss)


class NTXentLoss(Module):
    def __init__(self, temperature: float = 1.0) -> None:
        super().__init__()
        self.t = temperature
        
    def _sim(self, x: Tensor) -> Tensor:
        xn = x / x.norm()
        S = xn @ xn.T
        return S
        
    def forward(self, A: Tensor, P: Tensor, N: Tensor) -> Tensor:
        import matplotlib.pyplot as plt
        X = torch.concat([A, P, N])
        
        S = torch.exp(self._sim(X) / self.t)
        plt.imshow(S)
        plt.colorbar()
        plt.show()


class OwnContrastiveLoss(Module):
    def __init__(self, temperature: float = 1.0) -> None:
        super().__init__()
        self.t = temperature
    
    def forward(self, A: Tensor, P: Tensor, N: Tensor) -> Tensor:
        AP = (A * P).sum(1) / (A.norm(dim=1) * P.norm(dim=1))
        AN = (A * N).sum(1) / (A.norm(dim=1) * N.norm(dim=1))
        return torch.log(1 + torch.exp((-AP + AN) / self.t)).mean()
        
if __name__ == "__main__":
    batch_size = 64
    feature_size = 512
    A = torch.rand((batch_size, feature_size))
    P = torch.rand((batch_size, feature_size))
    N = torch.rand((batch_size, feature_size))
    loss_fcn = OwnContrastiveLoss()
    L = loss_fcn(A, P, N)
    print(L)
    print(L.shape)